<img src="public/Logo.png"/>

# ğŸ¦– DyNaGO â€“ Dynamic Natural Gesture Operations

DyNaGO is a real-time AI-powered, Human Computer Interface employing gesture recognition. It uses computer vision and machine learning to enable users to control their machines using natural, dynamic hand gesturesâ€”no special hardware required.

Whether for accessibility, low-interaction environments, or futuristic UI prototyping, DyNaGO delivers a lightweight, modular, and efficient solution for gesture-based computing.

---

## âœ¨ Features

- ğŸ”§ **SVM + MediaPipeâ€“based gesture classification**
- âš¡ **Dynamic velocity vector analysis** for real-time gesture detection
- ğŸ® **System command mapping**: volume control, tab switching, app launch, and more
- ğŸ–¥ï¸ **Fully functional on standard webcams**
- ğŸ§± **Modular architecture** â€“ easily expandable with new gestures or models
- ğŸ§ª **Trained on 4,200+ gesture samples** across 6 static classes

---

## ğŸ§  Dataset & Training Summary

- Total Samples: **4291**
- Gestures: `fist`, `two_fingers`, `three_fingers (2 types)`, `pinch`, `point`
- Normalization: **wrist-centered + scaled to unit sphere**
- Accuracy: **92.3%**
- Best Class: `point` (99.4%)
- Weakest Class: `pinch` (72.3%)

### Confusion Matrix Preview:

> <img src="dynago/performance/confusion_matrix.png"/>

---

## ğŸ— System Architecture

> <img src="public/architecture.png"/>

1. **Initialization** â€“ Load webcam, environment, set base gesture
2. **Static Gesture Detection** â€“ Classify using MediaPipe landmarks + SVM
3. **Motion Vector Analysis** â€“ Track gesture trajectory using velocity between frames
4. **Action Mapping** â€“ Trigger system functions via OS hooks / APIs

---

## ğŸ›  Usage

### Installation

```bash
git clone https://github.com/KreativeThinker/DyNaGO
cd DyNaGO
python -m venv .venv
source .venv/bin/activate
pip install poetry
poetry install
```

### Commands

| Command                   | Task                                       |
| ------------------------- | ------------------------------------------ |
| `poetry run capture`      | Capture training samples with label        |
| `poetry run normalize`    | Normalize and prepare dataset for training |
| `poetry run train_static` | Train SVM model                            |
| `poetry run dev`          | Launch dynamic gesture predictor           |

> \>\_ See all commands: [pyproject.toml](./pyproject.toml)

---

## ğŸ“ˆ Experiment Highlights

| Gesture         | Accuracy | AUC  | Confusions                              |
| --------------- | -------- | ---- | --------------------------------------- |
| `point`         | 99.4%    | 1.00 | minor confusion with `fist`             |
| `pinch`         | 72.3%    | 0.95 | major confusion with `palm` and `point` |
| `three_fingers` | 87.3%    | 1.00 | some confusion with `two_fingers`       |

> ğŸ“Š See full report: [Experiment Analysis](./Experiment_Analysis.md)

---

## ğŸ¥ Demo

_Coming Soon â€” recording in progress. Will showcase real-time gesture use for volume control and workspace switching._

---

## ğŸŒ± Future Work

- Better configuration file
- Hybrid dynamic gesture detection with light weight SVM + Velocity Vector Analysis
- Complete cursor control
- Real-time inference optimization (GPU support)
- Multi-gesture chaining (command macros)
- Browser-based version via TensorFlow.js
- Integrated Audio Agent with custom function execution (branch [voice](https://github.com/KreativeThinker/Dynago/tree/voice))

---

## ğŸ‘¨â€ğŸ’» Author

Built by [Anumeya Sehgal](https://github.com/KreativeThinker)  
âœ‰ Email: [anumeyasehgal@proton.me](mailto:anumeyasehgal@proton.me)  
ğŸŒ LinkedIn: [anumeya-sehgal](https://linkedin.com/in/anumeya-sehgal)

---

## ğŸ“œ License

[MIT License](./LICENSE) â€“ Free for use, distribution, and enhancement.
